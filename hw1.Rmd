---
title: "Homework 1"
author: "Jesse Weiss"
date: "January 28, 2018"
output: html_document
---
<style>
body{
background: white;
}
#header {
    margin: auto;
    width: 50%;
    text-align: center;}
h1 {
font-size:26px !important;
}
.li_1 {
padding-bottom: 10px;
}
</style>

```{r setup, include=FALSE}
library(ISLR)
data(Auto)
data(Carseats)
knitr::opts_chunk$set(echo = TRUE)
```
<ol>
<li> 
<p style = "margin-left: 20px; margin-top:20px; margin-bottom:30px;>
The null hypthesis for intercept, tv, radio and newspaper is that there is no relationship between each term and the dependent variable (sales). Additionally, the null hypthesis states that the coefficients for each of these variables are equal to zero. $$H_0 : \beta_0 = \beta_1 = \beta_2 = \beta_3 = 0$$
The approximately zero p-value for TV indicates that there is an extremely low probability that TV spend has no association with sales. The same can be said for radio, the near zero p-value indicates an extremely low probability that radio spend has no association with sales. The high p-value for newspaper indicates a lack of evidence that newspaper has any significant relationship with sales when radio and tv spend are considered and held constant.
</p>
</li>
<li value = '3'>
  <ol type = 'a'>
  <li class = 'li_1'> 
iii - For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.</li>
  <li class = 'li_1'><br/>
```{r}
b0 = 50; b1 = 20; b2 = .07; b3 = 35; b4 = .01; b5 = -10
(b0 + b1*4 + b2*110 + b3*1 + b4*4*110 + b5*4*1) * 1000
```
  </li>
  <li style = 'margin-bottom:30px;'> 
False because we don't have enough information to answer the question. We need to be able to compute a p value for  $\beta_4$ to properly reject or accept the null hypothesis.
  </li>
  </ol>
</li>
<li>
  <ol type = 'a'>
  <li class = 'li_1'> 
There isn't enough information to be entirely sure, but the cubic regression line might have a slightly lower RSS since it is more flexible than the linear regression. If there is very little irreducible error in the data though, it's possible that the linear regression would actually have a lower RSS.
  </li>
  <li class = 'li_1'> 
The linear regression is likely to have a better fit on the test set because there is a true linear realtionship and a cubic would likely have overfit the training examples.
  </li>
  <li class = 'li_1'>
It's hard to say given that we don't know how far from linear the relationship is but its more likely that the cubic regression will have a lower training RSS due to its greater flexibility. 
  </li>
  <li class = 'li_1'>
There really isn't enough data to answer this question confidently. If the releationship is extremely close to linear then the linear regression would likely have a lower test RSS. If the relationship is far away from linear there is no guarantee which type of model would have a lower RSS. THe high bias of the linear model and the greater varience of the cubic model could both lead to issues when given unseen data in this scenario.
  </li>
  </ol>
</li>
<li>
$$\hat{y_i} = \frac{\sum^n_{{i`} = 1}x_{i`} y_{i`}}{\sum^n_{j = 1}x^2_j}x_i=\sum^n_{{i`} = 1}\frac{x_{i`}x_i}{\sum^n_{j = 1}x^2_j}y_{i`} = \sum^n_{{i`} = 1}a_{i`}y_{i`}$$
<center>where</center>
$$a_{i`} = \sum^n_{{i`} = 1}\frac{x_{i`}x_i}{\sum^n_{j = 1}x^2_j}y_{i`}$$
</li>
<li>
Knowing the linear regression equation $y = \hat{\beta_0} + \hat{\beta_1}x$ and that $\hat{\beta_0} = \bar{y} - \hat{\beta}_1\bar{x}$, we can substitute $\bar{x}$ for $x$ and solve for $y$
$$y = \bar{y} - \hat{\beta}_1\bar{x} + \hat{\beta_1}\bar{x} = \bar{y}$$
</li>
<li>
Knowing that $$R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_i{(y_i - \hat{y}_i)}^2}{\sum_j{y^2_j}}$$
And that $$COR = \frac{(\sum_i{x_iy_i})^2}{(\sum_jx^2_j)(\sum_jy^2_j)}$$
We can say that $$1 - \frac{\sum_i{(y_i - \hat{y}_i)}^2}{\sum_j{y^2_j}} = \frac{(\sum_i{x_iy_i})^2}{(\sum_jx^2_j)(\sum_jy^2_j)}$$
</li>
<li> <br/>
```{r}
model <- lm(mpg~horsepower, data = Auto)
summary(model)
```
 <Ol type = 'a'>
 <li>
 <ol type = 'i'>
 <li class = 'li_1'>
With a p value of approximately 0, we can infer that there is a relationship between mpg and horsepower.
 </li>
 <li class = 'li_1'>
There is a strong relationship between mpg and horsepower.
 </li>
 <li class = 'li_1'>
The relationship between mpg and horsepower is negative.
 </li>
 <li class = 'li_1'><br/>
```{r}
predict(model, data.frame(horsepower = 98), interval = "prediction")
```
 </li>
 </ol>
 <li><br/>
```{r}
plot(Auto$horsepower, Auto$mpg)
abline(model)
```
</li>
 <li><br/>
```{r}
plot(model)
```
<br/>
There are some high leverage points in this dataset an a noticable pattern in the residuals which indicates that the relationship between mpg and horsepower may not be linear.
</li>
</ol>
</li>
<li>
<Ol type = 'a'>
<li><br />
```{r}
pairs(Auto)
```
</li>
<li><br/>
```{r}
cor(Auto[-9])
```
</li>
<li><br/>
```{r}
model <- lm(mpg~., data = Auto[-9])
summary(model)
```
<Ol type = 'i'>
<li> There is a realtionship between the predictors and response</li>
<li> displacement, weight, year and origin all seem to have statistically significant relationships with the reponse (mpg)</li>
<li>The coefficient for year (.750) suggestes that cars made in later years have higher mpg</li>
</ol>
</li>
<li><br />
```{r}
plot(model)
```
<br/>There is one points with very high leverage (row 14) and teh residuals plot suggests that there are a few large outliers, primarily resulting from vehicles with high mpgs.
</li>
<li><br/>
```{r}
model <- lm(mpg ~ . + displacement * weight + cylinders * horsepower, data = Auto[-9])
summary(model)
```
<br /> Displacement:weight and cylinders:horsepower both seem to be significant interactions, each having near 0 p-values.</li>
<li><br/>
```{r}
model <- lm(mpg ~ . + log(weight) + sqrt(cylinders), data = Auto[-9])
summary(model)
```
<br/>Log weight is a significant predictor that seems to add value to this model.
</li>
</ol>
</li>
<li>
<Ol type = 'a'>
<li><br/>
```{r}
model <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(model)
```
</li>
<li>
Price is a significant predictor in this model with a negative coefficient indicating that higher prices will decrease sales. Urban does not seem to be a significant predictor. US is significant and has a positive coefficient meaning that a value of yes for US should have  positive impact on sales.
</li>
<li>
$\hat{y} = 13.04 + -.05x_1 + -.02x_2 + 1.2x_3$
</li>
<li>
We may reject the null hypothesis for the Price and US predictors
</li>
<li><br/>
```{r}
model <- lm(Sales ~ Price + US, data = Carseats)
summary(model)
```
</li>
<li>
BOth models have significant predictors and explain some variance but the $R^2$ values indicate that much of the variance is still left unexplained by both models.
</li>
<li></br>
```{r}
confint(model, level = .95)
```
</li>
<li><br />
```{r}
plot(model)
```
<br/> There are a few high leverage points but no extreme outliers based on the residual and leverage plots of this model.
</li>
</ol>
</li>
</ol>