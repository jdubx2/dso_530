---
title: "hw3"
author: "Jesse Weiss"
date: "March 25, 2018"
output: html_document
---
<style>
body{
background: white;
}
#header {
    margin: auto;
    width: 50%;
    text-align: center;}
h1 {
font-size:26px !important;
}
.li_1 {
padding-bottom: 10px;
}
</style>

```{r setup, include=FALSE}
library(ISLR)
library(dplyr)
library(ggplot2)
library(boot)
library(leaps)
library(tidyr)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE)
```
<ol>
<li value='1'>
$$Var(aX + (1-a)Y)=Var(aX) + Var((1-a)Y) + 2Cov(aX,(1-a)Y)$$
$$= a^2Var(X) + (1-a)^2Var(Y) + 2a(1-a)Cov(X,Y)$$
$$f(a)=\sigma^2_Xa^2+\sigma^2_Y(1-a)^2+2\sigma_{XY}(-a^2+a)$$
Set the partial derivative of $f(a)$ = 0 and solve for $a$
$$0 = \frac{d}{da}f(a)$$
$$0 = 2\sigma^2_Xa+2\sigma^2_Y(1-a)(-1)+2\sigma_{XY}(-2a+1)$$
$$0=a(\sigma^2_X+\sigma^2_Y-2\sigma_{XY})-\sigma^2_Y+\sigma_{XY}$$
$$a=\frac{\sigma^2_Y-\sigma_{XY}}{\sigma^2_X+\sigma^2_Y-2\sigma_{XY}}$$
</li>
<li value = '3'>
  <ol>
  <li type = 'a'>
In $k$-fold cross-validation, the training data is first split into $k$ mutually exclusive partitions. The model is then fit $k$ times, each time using $k-1$ partitions as the test set and the remaining partition as the validation set. The model evaluation criteria are averaged accross all $k$ results.
  </li>
  <li type = 'a'><br />
  <ol><li type='i'>Compared to the validation set approach, $k$-fold cross-validation can be advantageous because it leverages the entire dataset for training (not just the pre-allocated testing sample). This makes $k$-fold less variable as the validation set approach will depend heavily on which observations are included in the test set. On the other hand, the validation set is simple to implement and demands less computing resources.</li>
  <li type = 'i'>LOOCV is a special case of $k$-fold cross validation where $k=n$. It can be disadvantageous to use LOOCV because it is computationaly intensive (model must be fit $n$ times) and has higher variance than a smaller choice for $k$.</li></ol>
  </li></ol>
<li value ='5'>
<ol><li type='a'><br />
``` {r}
attach(Default)
set.seed(1)

model_error <- function(){
  
  trainset <- sample(nrow(Default),nrow(Default)*.5)
  fit <- glm(default ~ income + balance, family = 'binomial', subset = trainset)
  test.prob <- predict(fit, newdata = Default[-trainset,], type = 'response')
  test.predict <- ifelse(test.prob > .5,'Yes','No')
  
  return(1-sum(test.predict == Default[-trainset,]$default)/length(test.predict))
}

model_error()
```
</li>
<li type = 'a'><br />
```{r}
model_error()
model_error()
model_error()
```
<p>Different training splits of this model produce similar results with test error rates ranging from 2.36% - 2.68%</p>
</li></ol>
</li>
<li value = '1'>
 <ol>
 <li type = 'a'>best subset selection should have the smallest training RSS because it requires fitting all $2^p$ possible models (evaluated by RSS) while forward and backward stepwise selection fit only a subset of these models.</li>
 <li type = 'a'>Best subset selection again ahs an advantage because it fits all models however there is no guarantee that a model with the best training performance will also have the smallest test RSS. Forward or backward stepwise may have a smaller test error by chance.</li>
 <li type = 'a'>
 <ol type = 'i'>
 <li>True</li>
 <li>True</li>
 <li>False</li>
 <li>False</li>
 <li>False</li>
 </ol>
 </li>
 </ol>
</li>
<li>
 <ol type = 'a'>
 <li>
  Option iii is correct. The lasso will shrink the coefficients of the model and perform variable selection thereby reducing the models variance and increasing its bias. If the decrease in variance is larger than the increase in bias the model should make better predictions.
 </li>
 <li>
  option iii is correct. Ridge will shrink the coefficients of the model thereby reducing the model's variance and increasing its bias. If the decrease in variance is alarger than the increase in bias the model should make better predictions.
 </li>
 </ol>
</li>
<li value = '8'>
 <ol type = 'a'>
 <li><br />
```{r}
set.seed(1)
X <- rnorm(100)
Ep <- rnorm(100)
```
 </li>
 <li><br />
```{r}
b0=1;b1=2;b2=3;b3=4
Y = b0 + b1*X + b2*X^2 + b3*X^3 + Ep
```
 </li>
 <li><br />
```{r}
df <- data.frame(x = X, y = Y)
regfit <- regsubsets(y~poly(x,10, raw = T),data =df, nvmax=10)

results <- data.frame(model = seq(1,10),
                      adjr2 = summary(regfit)$adjr2,
                      cp = summary(regfit)$cp,
                      bic = summary(regfit)$bic) %>% 
  gather(criterion,value,-model)

best <- results %>% group_by(criterion) %>% 
  mutate(minval = min(value), maxval = max(value),
         best = ifelse(criterion == 'adjr2', maxval,minval)) %>% 
  filter(value == best)

results %>% ggplot(aes(x = model, y = value, color = criterion))+
  geom_line()+ geom_point(size = 2,data=best) +
  facet_grid(criterion~., scales='free_y') +
  scale_x_continuous(breaks = seq(1,10)) + theme_bw()

coef(regfit, 3)

```
 <p>BIC is lowest for model 3 while adjusted $R^2$(max) and $C_p$(min) indicate model 4 as optimal. The extremely small improvement in both adj $R^2$ and $C_p$ from model 3 to 4 leads me to beleive that the extra predictor in model 4 is not adding much value to the predictive power of the model and could result in uneccessary variance. For these reasons I'm inclined to select model 3 as the best model.</p>
 </li>
 <li><br />
```{r}
regfit_fw <- regsubsets(y~poly(x,10, raw = T),data =df, nvmax=10, method = 'forward')
regfit_bw <- regsubsets(y~poly(x,10, raw = T),data =df, nvmax=10, method = 'backward')

results_sw <- rbind(data.frame(method = 'forward',
                      model = seq(1,10),
                      adjr2 = summary(regfit_fw)$adjr2,
                      cp = summary(regfit_fw)$cp,
                      bic = summary(regfit_fw)$bic),
                    data.frame(method = 'backward',
                      model = seq(1,10),
                      adjr2 = summary(regfit_bw)$adjr2,
                      cp = summary(regfit_bw)$cp,
                      bic = summary(regfit_bw)$bic))%>% 
  gather(criterion,value,-c(method,model))

best_sw <- results_sw %>% group_by(method,criterion) %>% 
  mutate(minval = min(value), maxval = max(value),
         best = ifelse(criterion == 'adjr2', maxval,minval)) %>% 
  filter(value == best)

results_sw %>% ggplot(aes(x = model, y = value, color = criterion))+
  geom_line()+ geom_point(size = 2,data=best_sw) +
  facet_grid(criterion~method, scales='free_y') +
  scale_x_continuous(breaks = seq(1,10)) + theme_bw()
```
<p>The results of forward and backward stepwise selection are very similar to the results of best subset selection. BIC is still optimal at the model with 3 predictors while adjusted $R^2$ and $C_p$ are optimal at the model with 4 predictors.</p>
 </li>
 <li><br />
```{r}
x2 <- model.matrix(y~poly(x,10,raw=T),df)[,-1]
fit_lasso <- cv.glmnet(x2,Y,alpha=1)
plot(fit_lasso)
best.lambda <- fit_lasso$lambda.min
best.lambda
fit_lasso_best <- glmnet(x2,Y,alph=1)
predict(fit_lasso_best, s = best.lambda, type = "coefficients")
```
  <p>The lasso kept $X, X^2, X^3, X^4, X^5, X^7$ in the model with the best choice of lambda.</p>
 </li>
 <li>
```{r}
b7=7
Y = b0+b7*X^7+Ep
df2 <- data.frame(y=Y, x=X)
fit <- regsubsets(y ~poly(X,10,raw=T),data=df2,nvmax=10)

results <- data.frame(model = seq(1,10),
                      adjr2 = summary(fit)$adjr2,
                      cp = summary(fit)$cp,
                      bic = summary(fit)$bic) %>% 
  gather(criterion,value,-model)

best <- results %>% group_by(criterion) %>% 
  mutate(minval = min(value), maxval = max(value),
         best = ifelse(criterion == 'adjr2', maxval,minval)) %>% 
  filter(value == best)

results %>% ggplot(aes(x = model, y = value, color = criterion))+
  geom_line()+ geom_point(size = 2,data=best) +
  facet_grid(criterion~., scales='free_y') +
  scale_x_continuous(breaks = seq(1,10)) + theme_bw()

coef(regfit, 4)
```
 With best subset selection, BIC selects the 1 variable model, adjusted $R^2$ picks the 4 variable model, and $C_p$ picks the 2 variable model.
```{r}
X2 = model.matrix(y ~ poly(x, 10, raw = T), data = df2)[, -1]
fit.lasso = cv.glmnet(X2, Y, alpha = 1)
best.lam = fit.lasso$lambda.min
best.lam
best.model = glmnet(X2, Y, alpha = 1)
predict(best.model, s = best.lam, type = "coefficients")
```
The lasso selects the best 1 variable model using $x^7$
 </li>
 </ol>
</li>
</ol>