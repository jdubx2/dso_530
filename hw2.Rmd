---
title: "DSO 530 HW2"
author: "Jesse Weiss"
date: "February 19, 2018"
output: html_document
---

<style>
body{
background: white;
}
#header {
    margin: auto;
    width: 50%;
    text-align: center;}
h1 {
font-size:26px !important;
}
.li_1 {
padding-bottom: 10px;
}
</style>

```{r setup, include=FALSE}
library(ISLR)
library(dplyr)
library(ggplot2)
library(tidyr)
library(MASS)
library(class)
data(Weekly)
knitr::opts_chunk$set(echo = TRUE)
```
<ol>
<li value='7'> 153/184
```{r}
(.8 * (1/sqrt(2*36)*exp(-(1/(2*36))*(4-10)^2))) / ((.8 * (1/sqrt(2*36)*exp(-(1/(2*36))*(4-10)^2))) + .2 * (1/sqrt(2*36)*exp(-(1/(2*36))*(4-0)^2)))

```
</li>
<li value='10'>
  <ol type = 'a'>
  <li><br />
```{r}
pairs(Weekly[-9])

summary(lm(log(Volume)~Year, Weekly))

cor(Weekly[-9])

Weekly %>% 
  dplyr::select(Lag1:Lag5, Direction) %>% 
  gather(lag,value,-Direction) %>% 
  ggplot(aes(x = lag, y = value, fill = Direction)) +
  geom_boxplot()

Weekly %>% 
  group_by(Year) %>% summarise(total_gain_loss = sum(Today)) %>% 
  ggplot(aes(x = Year, y = total_gain_loss))+ geom_line(color='deepskyblue2',size=1) +
  geom_point(size=1.5, color = 'gray30') + geom_hline(yintercept = 0, linetype = 'dashed') +
  scale_x_continuous(breaks = seq(1990,2010,2))

```
<br />The pairs plot and subsequent linear regrssion reveals a strong positive association between the Year and Volume varaibles.The trend is logarithimic in nature. We can also see through correlations and subsequent box plots that the lag1 varaible has a slightly negative relationship to the current day change while the lag2 variable has a slightly positive relationship with the current day. Plotting the total gain/loss by year for the entire dataset reveals that we are more likely to see gains than losses in aggregate.
  </li>
  <li> <br />
```{r}
sub = dplyr::select(Weekly, -c(Year,Today))
lr1 <- glm(Direction ~ ., data=sub, family = 'binomial')
summary(lr1)
```
  The only statistically significant predictor in this model appears to be Lag2 with a P value of .0296
  </li>
  <li><br/>
```{r}
lr1.prob <- predict(lr1, type = 'response')
lr1.predict <- ifelse(lr1.prob > .5, 'Up','Down')
table(lr1.predict,Weekly$Direction)
(54+557)/(54+48+430+557)
```
Using logistic regression with a probabiltity decision threshold of .5 predicts the direction correctly 56% of the time. This is slightly better than random. The confusion matrix indicates that the model tends to bias towards upwards predictions at this decision threshold.
  </li>
  <li><br />
```{r}
lr2 <- glm(Direction ~ Lag2, data = filter(Weekly, Year < 2009), family = 'binomial')
lr2.prob <- predict(lr2, newdata = filter(Weekly, Year >= 2009), type = 'response')
lr2.predict <- ifelse(lr2.prob > .5, 'Up','Down')
table(lr2.predict,filter(Weekly, Year >= 2009)$Direction)
(9+56)/(9+5+34+56)
```
  </li>
  <li><br />
```{r}
lda1 <- lda(Direction ~ Lag2, data = filter(Weekly, Year < 2009))
lda1.predict <- predict(lda1, newdata = filter(Weekly, Year >= 2009))
lda1.class <- lda1.predict$class
table(lda1.class, filter(Weekly, Year >= 2009)$Direction)
(9+56)/(9+5+34+56)
```
  </li>
  <li><br />
```{r}
qda1 <- qda(Direction ~ Lag2, data = filter(Weekly, Year < 2009))
qda1.predict <- predict(qda1, newdata = filter(Weekly, Year >= 2009))
qda1.class <- qda1.predict$class
table(qda1.class, filter(Weekly, Year >= 2009)$Direction)
61/(43+61)
```
  </li>
  <li><br />
```{r}
set.seed(1)
knn.pred1 <- knn(filter(Weekly, Year < 2009)[3],
                filter(Weekly, Year >= 2009)[3],
                filter(Weekly, Year < 2009)$Direction, k = 1)
table(knn.pred1, filter(Weekly, Year >= 2009)$Direction)
(21+31)/(30+22+21+31)
```
  </li>
  <li>
Logistic regression and LDA give us identical results in this example and perform the best with the lowest classification error. QDA has a lower classification error than KNN but predicts up for every data point which is undesired. KNN with k=1 performs the worst with .5 classification error.
  </li>
  </ol>
</li>
<li><br />
  <ol type = 'a'>
  <li><br />
```{r}
data(Auto)
Auto1 <- Auto %>% mutate(mpg01 = ifelse(mpg > median(mpg), 1, 0))
```
  </li>
  <li><br />
```{r}
pairs(Auto1[-9])

Auto1 %>% dplyr::select(displacement:acceleration,mpg01) %>% 
  gather(stat, value, -mpg01) %>% 
  ggplot(aes(x = '', y = value, fill = as.factor(mpg01)))+
  geom_boxplot() +
  facet_grid(stat~., scales = 'free_y')
```
<br/>Displacement, horsepower and weight all look like they will be hlepful in predicting the value of mpg01.
  </li>
  <li></br>
```{r}
auto_train <- Auto1[1:200,]
auto_test <- Auto1[201:392,]
```
  </li>
  <li><br />
```{r}
lda <- lda(mpg01 ~ displacement + horsepower + weight, data = auto_train)
lda.predict <- predict(lda, newdata = auto_test)
lda.class <- lda.predict$class
table(lda.class, auto_test$mpg01)
(11+12)/(52+11+12+117)
```
  <br /> LDA test error is <b>12.0%</b>
  </li>
  <li><br />
```{r}
qda <- qda(mpg01 ~ displacement + horsepower + weight, data = auto_train)
qda.predict <- predict(qda, newdata = auto_test)
qda.class <- qda.predict$class
table(qda.class, auto_test$mpg01)
(7+19)/(57+19+7+109)
```
  <br /> QDA test error is <b>13.5%</b>
  </li>
  <li><br />
```{r}
glm <- glm(mpg01 ~ displacement + horsepower + weight, data = auto_train, family = 'binomial')
glm.probs <- predict(glm, newdata = auto_test, type = 'response')
glm.predict <- ifelse(glm.probs > .5, 1, 0)
table(glm.predict, auto_test$mpg01)
(35+2)/(62+93+2+35)
```
    <br /> Logistic regression test error is <b>19.3%</b>
  </li>
  <li><br />
```{r}
set.seed(1)
knn.pred <- knn(auto_train[,3:5],
                auto_test[,3:5],
                auto_train$mpg01, k = 1)
table(knn.pred, auto_test$mpg01)
```
  </li>
  </ol>
</li>